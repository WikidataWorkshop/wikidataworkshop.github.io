We propose a simple Named Entity Linking system that can be trained from Wikidata only. This demonstrates the strengths and weaknesses of this data source for this task and provides an easily reproducible baseline to compare other
systems against. Our model is lightweight to train, to run and to keep synchronous
with Wikidata in real time.

New dataset about >25000 entries of "Biographisches Lexikon des Kaiserthums Oesterreich", a 19th century biographical dictionary of the Austrian Empire by Constant von Wurzbach, including 12000 people previously not mentioned in Wikipedia/Wikidata and main reference works cited in the dictionary. It enhances the Wikisource transcription of the work and provides a comparison with other datasets

Abstract Wikipedia is a Wikimedia Foundation project comprised of two repositories, one of functions and another of abstract content. Its goal is to collaboratively create functions that convert content from multiple Wikipedia language editions into an abstract language, allowing editors to maintain it, and feeding it back to each of these language editions so they can expand more rapidly and efficiently. In this paper, we argue that it is necessary to design the functions and user interfaces so that they encourage contributors to actively work on content diversity across Wikipedia language editions. Drawing inspiration from different Wikipedia studies on editing dynamics and content contextualisation, we suggest six design goals and six requirements: topic identification, gap bridging, perspective modelling, items representation, article composition, and dispute visualization. Finally, we discuss the importance of helping smaller languages grow and fostering knowledge diversity through the exchange of existing contents.

Version control systems provide ways to ease collaboratively working on a set of source files. Due to the nature of RDF models that represent ontologies, there is a clear benefit on using those kinds of systems, and many well-known datasets and vocabularies are already being worked on collaboratively. However, there are domain experts who could provide feedback and add new knowledge to the data, but do not know how to work with version control systems or with RDF files directly. Wikibase provides a human-friendly interface with an underlying Linked Data model, and can be used as a publication service for the data where those experts can easily browse and add their knowledge. In this paper, we propose a system that automatically synchronizes RDF files hosted in a version control system with a Wikibase instance. We describe the system from an architectural point of view, and explain the main components needed for the synchronization of data. Some of the challenges for an effective synchronization of RDF files with Wikibase are also addressed. This system is currently being used as part of the HÃ©rcules project to synchronize a research ecosystem ontology with a Wikibase instance.

Wikidata is steadily becoming more central to Wikipedia, not just in maintaining interlanguage links, but in automated population of content within the articles themselves. It is not well understood, however, how widespread this transclusion of Wikidata content is within Wikipedia. This work presents a taxonomy of Wikidata transclusion from the perspective of its potential impact on readers and an associated in-depth analysis of Wikidata transclusion within English Wikipedia. It finds that Wikidata transclusion that impacts the content of Wikipedia articles happens at a much lower rate (5%) than previous statistics had suggested (61%). Recommendations are made for how to adjust current tracking mechanisms of Wikidata transclusion to better support metrics and patrollers in their evaluation of Wikidata transclusion.

Due to its numerous bibliometric entries of scholarly articles and connected information Wikidata can serve as an open and rich source for deep scientometrical analyses. However, there are currently certain limitations: While 31.5\% of all Wikidata entries represent scientific articles, only 8.9\% are entries describing a person and the number of entries researcher is accordingly even lower. Another issue is the frequent absence of established relations between the scholarly article item and the author item although the author is already listed in Wikidata. To fill this gap and to improve the content of Wikidata in general, we established a workflow for matching authors and scholarly publications by integrating data from the ORCID (Open Researcher and Contributor ID) database. By this approach we were able to extend Wikidata by more than 12k author-publication relations and the method can be transferred to other enrichments based on ORCID data. This is extension is beneficial for Wikidata users performing bibliometrical analyses or using such metadata for other purposes.

Wikidata and Wikipedia have been proven useful for reasoning in natural language applications, like question answering or entity linking. Yet, no existing work has studied the potential of Wikidata for commonsense reasoning. This paper investigates whether Wikidata contains commonsense knowledge which is complementary to existing commonsense sources. Starting from a definition of common sense, we devise three guiding principles, and apply them to generate a commonsense subgraph of Wikidata (Wikidata-CS). Within our approach, we map the relations of Wikidata to ConceptNet, which we also leverage to integrate Wikidata-CS into an existing consolidated commonsense graph. Our experiments reveal that: 1) albeit Wikidata-CS represents a small portion of Wikidata, it is an indicator that Wikidata contains relevant commonsense knowledge, which can be mapped to 15 ConceptNet relations; 2) the overlap between Wikidata-CS and other commonsense sources is low, motivating the value of knowledge integration; 3) Wikidata-CS has been evolving over time at a slightly slower rate compared to the overall Wikidata, indicating a possible lack of focus on commonsense knowledge. Based on these findings, we propose three recommended actions to improve the coverage and quality of Wikidata-CS further.

Wikidata constraints, albeit useful, are represented and processed in
an incomplete, ad hoc fashion. Constraint declarations do not fully
express their meaning, and thus do not provide a precise, unambiguous
basis for constraint specification, or a logical foundation for
constraint-checking implementations. In prior work we have proposed a
logical framework for Wikidata as a whole, based on multi-attributed
relational structures (MARS) and related logical languages. In this
paper we explain how constraints are handled in the proposed
framework, and show that nearly all of Wikidata's existing property
constraints can be completely characterized in it, in a natural and
economical fashion. We also give characterizations for several
proposed property constraints, and show that a variety of non-property
constraints can be handled in the same framework.

Wikidata is a key resource for the provisioning of structured data on several Wikimedia projects, including Wikipedia. By design, all Wikipedia articles are linked to Wikidata entities; such mappings represent a substantial source of both semantic and structural information. However, only a small subgraph of Wikidata is mapped in that way - only about 10% of the sitelinks are linked to English Wikipedia, for example. In this paper, we describe a resource we have built and published to extend this subgraph and add more links between Wikidata and Wikipedia. We start from the assumption that a number of Wikidata entities can be mapped onto Wikipedia sections, in addition to Wikipedia articles. The resource we put forward contains tens of thousands of such mappings, hence considerably enriching the highly structured Wikidata graph with encyclopedic knowledge from Wikipedia.

Given a Wikidata claim, we explore automated methods for locating references that support that claim. Our goal is to assist human editors in referencing claims, and thus increase the ratio of referenced claims in Wikidata. As an initial approach, we mine links from the references section of English Wikipedia articles, download and index their content, and use standard relevance-based measures to find supporting documents. We consider various forms of search phrasings, as well as different scopes of search. We evaluate our methods in terms of the coverage of reference documents collected from Wikipedia. We also develop a gold standard of sample items for evaluating the relevance of suggestions. Our results in general reveal that the coverage of Wikipedia reference documents for claims is quite low, but where a reference document is available, we can often suggest it within the first few results.

We investigate global measures of vertex similarity for knowledge graphs. While vertex similarity has been explored in the context of directed, unlabelled graphs, measures based on recursive algorithms or learning frameworks can be costly to compute, assume labelled data, and/or provide poorly-interpretable results. Knowledge graphs further imply unique challenges for vertex similarity in terms of scale and diversity. We thus propose and explore global measures of vertex similarity for Knowledge Graphs that (i) are unsupervised, (ii) offer explanations of similarity results; (iii) take into consideration edge labels; and (iv) are robust in terms of redundant or interdependent information. Given that these measures can still be costly to compute precisely, we propose an approximation strategy that enables computation at scale. We compare our measures with a recursive measure (SimRank) for computing vertex similarity over subsets of Wikidata.

Data matching is a central part of many contribution workflows in Wikidata. We present a reconciliation service for Wikidata, implementing a standard API that is supported by other data providers and consumed by multiple clients. We explain the technical choices behind the architecture of the service and review its usage patterns in 2019.